{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d24d3416-f7b6-4591-8099-09d1a1ed3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98afe696-09f8-422c-a166-4857827adba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac53dd0-0dfc-4be9-a44f-7a468be32b02",
   "metadata": {},
   "source": [
    "# Data loading and basic preprocessing\n",
    "* removing special characters, extra spaces, and converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e354da93-ced2-46db-b73f-79393f8fe725",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Data processing steps\u001b[39;00m\n\u001b[0;32m     32\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m load_raw_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/temp_dataset.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m save_processed_data(processed_data, output_file)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# print(f\"Processed data saved to {output_file}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[60], line 19\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSanskrit Anuvad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHindi Anuvad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish Translation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExplanation\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 19\u001b[0m         df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace NaN with an empty string\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[60], line 11\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove extra spaces\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove special characters\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstop_words\u001b[49m]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "def load_raw_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the raw dataset.\"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text by removing special characters, stopwords, and lemmatizing.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "        words = [lemmatizer.lemmatize(word.lower()) for word in text.split() if word.lower() not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess the dataset: clean specific columns and handle missing values.\"\"\"\n",
    "    for col in ['Sanskrit Anuvad', 'Hindi Anuvad', 'English Translation', 'Explanation']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"\").apply(clean_text)  # Replace NaN with an empty string\n",
    "    return df\n",
    "\n",
    "def save_processed_data(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"Save the processed dataset to a file.\"\"\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    input_file = \"data/raw/verses_original.csv\"\n",
    "    output_file = \"data/processed/verses_processed.csv\"\n",
    "\n",
    "    # Data processing steps\n",
    "    raw_data = load_raw_data('data/raw/temp_dataset.xlsx')\n",
    "    processed_data = preprocess_data(raw_data)\n",
    "    save_processed_data(processed_data, output_file)\n",
    "    # print(f\"Processed data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e471ab3-3d54-43f3-983c-ad8221313843",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233148a3-5cf8-4a00-8f65-c7e0ba5e005f",
   "metadata": {},
   "source": [
    "# generating embeddings for content-nased filetering using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "06b8b57d-41f5-4bfb-bc92-dc0da941e10f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_embeddings() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m load_processed_data(input_file)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m embeddings, tfidf_model \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExplanation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Save embeddings and model\u001b[39;00m\n\u001b[0;32m     42\u001b[0m save_embeddings(embeddings, tfidf_model, embeddings_file, model_file)\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_embeddings() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_processed_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the processed dataset.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def generate_tfidf_embeddings(df: pd.DataFrame, column: str) -> np.ndarray:\n",
    "    \"\"\"Generate TF-IDF embeddings for a specific column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        max_df=0.8,   # Ignore terms that appear in more than 80% of documents\n",
    "        min_df=2,     # Ignore terms that appear in fewer than 2 documents\n",
    "        ngram_range=(1, 2)  # Use unigrams and bigrams\n",
    "    )\n",
    "    embeddings = vectorizer.fit_transform(df[column].fillna(\"\"))\n",
    "    return embeddings, vectorizer\n",
    "\n",
    "def generate_embeddings(text_list):\n",
    "    return model.encode(text_list, convert_to_tensor=True)\n",
    "\n",
    "def save_embeddings(embeddings, model, embeddings_file: str, model_file: str):\n",
    "    \"\"\"Save the embeddings and TF-IDF model.\"\"\"\n",
    "    np.save(embeddings_file, embeddings.toarray())\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    input_file = \"data/processed/verses_processed.csv\"\n",
    "    embeddings_file = \"models/content_based/verse_embeddings.npy\"\n",
    "    model_file = \"models/content_based/tfidf_model.pkl\"\n",
    "\n",
    "    # Load processed data\n",
    "    processed_data = load_processed_data(input_file)\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings, tfidf_model = generate_embeddings(processed_data, 'Explanation')\n",
    "\n",
    "    # Save embeddings and model\n",
    "    save_embeddings(embeddings, tfidf_model, embeddings_file, model_file)\n",
    "    print(f\"Embeddings saved to {embeddings_file}\")\n",
    "    print(f\"TF-IDF model saved to {model_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0cbd3-9057-4349-9650-912438095428",
   "metadata": {},
   "source": [
    "# content based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c25b2071-4815-40a8-87fa-78aa6ea98636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Title    Chapter       Verse  \\\n",
      "18  Arjuna's Vishada Yoga  Chapter 1  Verse 1.19   \n",
      "8   Arjuna's Vishada Yoga  Chapter 1   Verse 1.9   \n",
      "1   Arjuna's Vishada Yoga  Chapter 1   Verse 1.2   \n",
      "2   Arjuna's Vishada Yoga  Chapter 1   Verse 1.3   \n",
      "3   Arjuna's Vishada Yoga  Chapter 1   Verse 1.4   \n",
      "\n",
      "                                          Explanation  similarity_score  \n",
      "18                                                NaN               0.0  \n",
      "8   duryodhana emphasizes that many other brave wa...               0.0  \n",
      "1   sanjay describes how duryodhana upon seeing th...               0.0  \n",
      "2   duryodhana points out to dronacharya the great...               0.0  \n",
      "3   duryodhana highlights the presence of great wa...               0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_embeddings(embeddings_file: str, model_file: str):\n",
    "    \"\"\"Load pre-saved embeddings and the TF-IDF model.\"\"\"\n",
    "    embeddings = np.load(embeddings_file)\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return embeddings, model\n",
    "\n",
    "def find_similar_verses(user_input: str, tfidf_model, embeddings, data_file: str, top_n: int = 5):\n",
    "    \"\"\"Find top-N similar verses to the user's input.\"\"\"\n",
    "    # Load the processed dataset\n",
    "    data = pd.read_csv(data_file)\n",
    "    \n",
    "    # Combine multiple fields for similarity (e.g., Explanation + Keywords Tags)\n",
    "    combined_texts = data['Explanation'] + \" \" + data['Keywords Tags']\n",
    "    embeddings = tfidf_model.fit_transform(combined_texts.fillna(\"\"))\n",
    "\n",
    "    # Transform user input\n",
    "    user_vector = tfidf_model.transform([user_input])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(user_vector, embeddings).flatten()\n",
    "\n",
    "    # Get top-N similar verses\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    recommendations = data.iloc[top_indices]\n",
    "    recommendations['similarity_score'] = similarities[top_indices]\n",
    "\n",
    "    return recommendations[['Title', 'Chapter', 'Verse', 'Explanation', 'similarity_score']]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    embeddings_file = \"models/content_based/verse_embeddings.npy\"\n",
    "    model_file = \"models/content_based/tfidf_model.pkl\"\n",
    "    data_file = \"data/processed/verses_processed.csv\"\n",
    "    \n",
    "    # User input\n",
    "    # user_input = \"I am facing moral dilemmas in my decisions at work.\"\n",
    "    user_input = \"I am not feeling happy because of personal relationship issues\"\n",
    "    \n",
    "    # Load embeddings and TF-IDF model\n",
    "    embeddings, tfidf_model = load_embeddings(embeddings_file, model_file)\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = find_similar_verses(user_input, tfidf_model, embeddings, data_file)\n",
    "    print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bda6a51e-f221-46f4-bec7-61d150b7a3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Verse</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.9</td>\n",
       "      <td>duryodhana emphasizes that many other brave wa...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.2</td>\n",
       "      <td>sanjay describes how duryodhana upon seeing th...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.3</td>\n",
       "      <td>duryodhana points out to dronacharya the great...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.4</td>\n",
       "      <td>duryodhana highlights the presence of great wa...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Title    Chapter       Verse  \\\n",
       "18  Arjuna's Vishada Yoga  Chapter 1  Verse 1.19   \n",
       "8   Arjuna's Vishada Yoga  Chapter 1   Verse 1.9   \n",
       "1   Arjuna's Vishada Yoga  Chapter 1   Verse 1.2   \n",
       "2   Arjuna's Vishada Yoga  Chapter 1   Verse 1.3   \n",
       "3   Arjuna's Vishada Yoga  Chapter 1   Verse 1.4   \n",
       "\n",
       "                                          Explanation  similarity_score  \n",
       "18                                                NaN               0.0  \n",
       "8   duryodhana emphasizes that many other brave wa...               0.0  \n",
       "1   sanjay describes how duryodhana upon seeing th...               0.0  \n",
       "2   duryodhana points out to dronacharya the great...               0.0  \n",
       "3   duryodhana highlights the presence of great wa...               0.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc319f-e83e-4e59-bb4c-5e83fc4e2991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
